{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7898242,"sourceType":"datasetVersion","datasetId":4638248},{"sourceId":7918888,"sourceType":"datasetVersion","datasetId":4653219}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport argparse\nimport pickle\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport time\n\ndef one_hot_encode(labels, num_classes):\n    return torch.nn.functional.one_hot(labels, num_classes=num_classes)\n\ndef load_data(data_path, batch_size, num_workers):\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),  \n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  \n        transforms.RandomRotation(degrees=10),  \n        transforms.GaussianBlur(kernel_size=3),  \n        transforms.Resize((256, 256)),  \n        transforms.ToTensor(),  \n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))   \n    ])\n\n    data_root = data_path\n    \n    print(\"Starting to load data...\")\n\n    # Create ImageFolder datasets for train, val, and test\n    train_dataset = ImageFolder(root=data_root + '/train', transform=transform)\n    val_dataset = ImageFolder(root=data_root + '/val', transform=transform)\n    test_dataset = ImageFolder(root=data_root + '/test', transform=transform)\n\n    # Create DataLoaders for train, val, and test\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    # Start a separate thread to display a spinning bar\n#     def show_spinner():\n#         spinner = ['-', '\\\\', '|', '/']\n#         idx = 0\n#         while True:\n#             print(f\"\\rData Loading in Process... {spinner[idx % len(spinner)]}\", end=\"\", flush=True)\n#             idx += 1\n#             time.sleep(0.1)\n\n#     spinner_thread = threading.Thread(target=show_spinner)\n#     spinner_thread.daemon = True\n#     spinner_thread.start()\n    \n    class_labels_dict = {class_name: label for label, class_name in enumerate(train_dataset.classes, start=0)}\n    reverse_class_labels_dict = {label: class_name for class_name, label in class_labels_dict.items()}\n    \n    print(\"\\n\")\n    for class_name in class_labels_dict:\n        print(f\"{class_name} has Class Index {class_labels_dict[class_name]}\\n\")\n\n    for data, labels in train_loader:\n        one_hot_labels = one_hot_encode(labels, num_classes=25)\n            \n    print(\"\\nData loading completed.\")\n            \n    return train_loader, val_loader, test_loader\n\n# Example usage:\ndata_path = \"/kaggle/input/data-files/Birds_25\"\nbatch_size = 32\nnum_workers = 4\ntrain_loader, val_loader,test_loader = load_data(data_path, batch_size, num_workers)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-23T03:34:30.033382Z","iopub.execute_input":"2024-03-23T03:34:30.033767Z","iopub.status.idle":"2024-03-23T03:37:31.399811Z","shell.execute_reply.started":"2024-03-23T03:34:30.033717Z","shell.execute_reply":"2024-03-23T03:37:31.398634Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Starting to load data...\n\n\nAsian-Green-Bee-Eater has Class Index 0\n\nBrown-Headed-Barbet has Class Index 1\n\nCattle-Egret has Class Index 2\n\nCommon-Kingfisher has Class Index 3\n\nCommon-Myna has Class Index 4\n\nCommon-Rosefinch has Class Index 5\n\nCommon-Tailorbird has Class Index 6\n\nCoppersmith-Barbet has Class Index 7\n\nForest-Wagtail has Class Index 8\n\nGray-Wagtail has Class Index 9\n\nHoopoe has Class Index 10\n\nHouse-Crow has Class Index 11\n\nIndian-Grey-Hornbill has Class Index 12\n\nIndian-Peacock has Class Index 13\n\nIndian-Pitta has Class Index 14\n\nIndian-Roller has Class Index 15\n\nJungle-Babbler has Class Index 16\n\nNorthern-Lapwing has Class Index 17\n\nRed-Wattled-Lapwing has Class Index 18\n\nRuddy-Shelduck has Class Index 19\n\nRufous-Treepie has Class Index 20\n\nSarus-Crane has Class Index 21\n\nWhite-Breasted-Kingfisher has Class Index 22\n\nWhite-Breasted-Waterhen has Class Index 23\n\nWhite-Wagtail has Class Index 24\n\n\nData loading completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass IdentityNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(IdentityNorm, self).__init__()\n        self.momentum = momentum\n        self.running_mean = 0\n        self.running_var = 0\n        self.eps = torch.tensor(eps)\n        self.num_features = num_features\n        shape = (1, self.num_features, 1, 1)\n        self.gamma = nn.Parameter(torch.empty(shape))\n        self.beta = nn.Parameter(torch.empty(shape))\n\n        self._param_init()\n\n    def _param_init(self):\n        nn.init.zeros_(self.beta)\n        nn.init.ones_(self.gamma)\n\n    def forward(self, x):\n        return x\n\n\nclass StandardBatchNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(StandardBatchNorm, self).__init__()\n        self.momentum = momentum\n        self.running_mean = 0\n        self.running_var = 0\n        self.eps = torch.tensor(eps)\n        self.num_features = num_features\n        shape = (1, self.num_features, 1, 1)\n        self.gamma = nn.Parameter(torch.empty(shape))\n        self.beta = nn.Parameter(torch.empty(shape))\n\n        self._param_init()\n\n    def _param_init(self):\n        nn.init.zeros_(self.beta)\n        nn.init.ones_(self.gamma)\n\n    def forward(self, x):\n        if self.training:\n            n = x.numel() / x.size(1)\n            dimensions = (0, 2, 3)\n            var = x.var(dim=dimensions, keepdim=True, unbiased=False)\n            mean = x.mean(dim=dimensions, keepdim=True)\n\n            with torch.no_grad():\n                self.running_mean = self.momentum * mean + (1 - self.momentum) * self.running_mean\n                self.running_var = self.momentum * (n / (n - 1)) * var + (1 - self.momentum) * self.running_var\n\n        else:\n            mean = self.running_mean\n            var = self.running_var\n        dn = torch.sqrt(var + self.eps)\n        x = (x - mean) / dn\n\n        x = x * self.gamma + self.beta\n\n        return x\n\n\nclass InstanceNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(InstanceNorm, self).__init__()\n        self.momentum = momentum\n        self.running_mean = 0\n        self.running_var = 0\n        self.eps = torch.tensor(eps)\n        self.num_features = num_features\n        shape = (1, self.num_features, 1, 1)\n\n\n        self.gamma = nn.Parameter(torch.empty(shape))\n        self.beta = nn.Parameter(torch.empty(shape))\n\n        self._param_init()\n\n\n    def _param_init(self):\n        nn.init.zeros_(self.beta)\n        nn.init.ones_(self.gamma)\n\n\n    def forward(self, x):\n        N, C, H, W = x.shape\n\n        assert C == self.num_features\n        dimensions = (2,3)\n        if self.training:\n            mean = x.mean(dim=dimensions, keepdim=True)\n            var = x.var(dim=dimensions, keepdim=True)\n        else:            \n            mean = x.mean(dim=dimensions, keepdim=True)\n            var = x.var(dim=dimensions, keepdim=True)\n        dn = torch.sqrt(var + self.eps)\n        x = (x - mean)/ dn\n        x = x * self.gamma + self.beta\n\n        return x\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(LayerNorm, self).__init__()\n        self.eps = torch.tensor(eps)\n        self.num_features = num_features\n        shape = (1, self.num_features, 1, 1)\n\n        self.gamma = nn.Parameter(torch.empty(shape))\n        self.beta = nn.Parameter(torch.empty(shape))\n\n        self._param_init()\n\n\n    def _param_init(self):\n        nn.init.zeros_(self.beta)\n        nn.init.ones_(self.gamma)\n\n    def forward(self, x):\n        \n        N, C, H, W = x.shape\n\n        assert C == self.num_features\n        dimensions = (1,2,3)\n        if self.training:\n            mean = x.mean(dim=dimensions, keepdim=True)            \n            var = x.var(dim=dimensions, keepdim=True)\n        else:\n            mean = x.mean(dim=dimensions, keepdim=True)            \n            var = x.var(dim=dimensions, keepdim=True)\n        dn = torch.sqrt(var + self.eps)\n        x = (x - mean)/ dn\n\n        x = x * self.gamma + self.beta\n\n        return x\n\n\n\nclass GroupNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, group=4):\n        super(GroupNorm,self).__init__()\n        self.eps = torch.tensor(eps)\n        self.num_features = num_features\n        self.group = group        \n        shape = (1, self.num_features, 1, 1)\n\n\n        self.gamma = nn.Parameter(torch.empty(shape))\n        self.beta = nn.Parameter(torch.empty(shape))\n\n        self._param_init()\n\n\n    def _param_init(self):\n        nn.init.zeros_(self.beta)\n        nn.init.ones_(self.gamma)\n\n    def forward(self, x):\n        N, C, H, W = x.shape\n        assert C % self.group == 0\n        assert self.num_features == C\n\n        x = x.view(N, self.group, int(C / self.group), H, W)\n        dimensions = (1,2,3)\n        mean = x.mean(dim=dimensions, keepdim=True)\n        var = x.var(dim=dimensions, keepdim=True)\n        dn = torch.sqrt(var + self.eps)\n        x = (x - mean)/ dn\n        x = x.view(N, C, H, W)\n        \n        x = x * self.gamma + self.beta\n\n        return x\n\nclass BatchInstanceNorm(nn.Module):\n    def __init__(self, num_features, momentum = 0.1, eps=1e-5, rho=0.5):\n        super(BatchInstanceNorm, self).__init__()\n        self.momentum = momentum\n        self.running_mean = 0\n        self.running_var = 0\n        self.eps = torch.tensor(eps)\n        self.num_features = num_features\n        self.rho = rho\n        shape = (1, self.num_features, 1, 1)\n\n\n        self.gamma = nn.Parameter(torch.empty(shape))\n        self.beta = nn.Parameter(torch.empty(shape))\n\n        self._param_init()\n\n\n    def _param_init(self):\n        nn.init.zeros_(self.beta)\n        nn.init.ones_(self.gamma)\n\n    \n    def forward(self, x):\n        if self.training:            \n                \n            n = x.numel() / x.size(1)\n            dimensions = (0,2,3)\n            var_bn = x.var(dim=dimensions, keepdim=True, unbiased=False)\n            mean_bn = x.mean(dim=dimensions, keepdim=True)\n\n            with torch.no_grad():\n                \n                self.running_mean = self.momentum * mean_bn + (1 - self.momentum) * self.running_mean\n                self.running_var = self.momentum * (n/(n-1)) * var_bn + (1 - self.momentum) * self.running_var\n\n        else:\n            mean_bn = self.running_mean\n            var_bn = self.running_var\n        dn = torch.sqrt(var_bn + self.eps)\n        x_bn = (x - mean_bn)/ dn\n        dimensions = (2,3)\n        mean_in = x.mean(dim=dimensions, keepdim=True)\n        var_in = x.var(dim=dimensions, keepdim=True)\n        dn = torch.sqrt(var_in + self.eps)\n        x_in = (x - mean_in)/ dn\n\n        x = self.rho * x_bn + (1-self.rho) * x_in\n\n        x = x * self.gamma + self.beta\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:31.402032Z","iopub.execute_input":"2024-03-23T03:37:31.402350Z","iopub.status.idle":"2024-03-23T03:37:31.463824Z","shell.execute_reply.started":"2024-03-23T03:37:31.402323Z","shell.execute_reply":"2024-03-23T03:37:31.463050Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import time \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom tqdm import tqdm \nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt \n# from resnet import ResNet, Block \nimport argparse\nimport pickle\nimport os ","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:31.465191Z","iopub.execute_input":"2024-03-23T03:37:31.465544Z","iopub.status.idle":"2024-03-23T03:37:32.781020Z","shell.execute_reply.started":"2024-03-23T03:37:31.465514Z","shell.execute_reply":"2024-03-23T03:37:32.780009Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n# from norm import StandardBatchNorm, InstanceNorm, BatchInstanceNorm, LayerNorm, GroupNorm, IdentityNorm\n\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, stride=1, norm_type='bn'):\n        super().__init__()\n        self.expansion = 2\n        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        \n        if norm_type == 'bn':\n            self.nl1 = StandardBatchNorm(intermediate_channels)\n            self.nl2 = StandardBatchNorm(intermediate_channels* self.expansion)\n        elif norm_type == 'in':\n            self.nl1 = InstanceNorm(intermediate_channels)\n            self.nl2 = InstanceNorm(intermediate_channels* self.expansion)\n        elif norm_type == 'gn':\n            self.nl1 = GroupNorm(num_features=intermediate_channels)  # Assuming a group size of 4\n            self.nl2 = GroupNorm(num_features=intermediate_channels* self.expansion)\n        elif norm_type == 'bin':\n            self.nl1 = BatchInstanceNorm(intermediate_channels)\n            self.nl2 = BatchInstanceNorm(intermediate_channels* self.expansion)\n        elif norm_type == 'ln':\n            self.nl1 = LayerNorm(num_features=intermediate_channels)\n            self.nl2 = LayerNorm(num_features=intermediate_channels* self.expansion)\n        elif norm_type=='nn':\n            self.nl1= IdentityNorm(intermediate_channels)\n            self.nl2 = IdentityNorm(intermediate_channels* self.expansion)\n            \n        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False)    \n        self.relu = nn.ReLU()\n        self.identity_downsample = identity_downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x.clone()\n\n        x = self.conv1(x)\n        x = self.nl1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.nl2(x)\n\n        if self.identity_downsample is not None:\n            identity = self.identity_downsample(identity)\n\n        x += identity\n        x = self.relu(x)\n        return x\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, image_channels, num_classes, norm_type):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv1 = nn.Conv2d(image_channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if norm_type == 'bn':\n            self.norm = StandardBatchNorm(16)\n            self.gamma = self.norm.gamma\n            self.beta = self.norm.beta\n            \n        elif norm_type == 'in':\n            self.norm = InstanceNorm(16)\n            self.gamma = self.norm.gamma\n            self.beta = self.norm.beta\n            \n        elif norm_type == 'gn':\n            self.norm = GroupNorm(group=4, num_features=16)  # Assuming a group size of 4\n            self.gamma = self.norm.gamma\n            self.beta = self.norm.beta\n            \n        elif norm_type == 'bin':\n            self.norm = BatchInstanceNorm(16)\n            self.gamma = self.norm.gamma\n            self.beta = self.norm.beta\n            \n        elif norm_type == 'ln':\n            self.norm = LayerNorm(16)\n            self.gamma = self.norm.gamma\n            self.beta = self.norm.beta\n            \n        elif norm_type=='nn':\n            self.norm= IdentityNorm(16)\n            self.gamma = self.norm.gamma\n            self.beta = self.norm.beta\n        else:\n            raise ValueError(\"Invalid normalization type. Choose from 'batch', 'instance', or 'group'.\")\n        self.relu = nn.ReLU()\n        \n        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=16, stride=1, norm_type=norm_type)\n        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=32, stride=2, norm_type=norm_type)\n        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=64, stride=2, norm_type=norm_type)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((8,8))\n        self.fc = nn.Linear(128 * 8*8, num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc(x)\n\n        return x\n    \n    \n    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride, norm_type):\n        identity_downsample = None\n        layers = []\n\n        if stride != 1 or self.in_channels != intermediate_channels * 2:\n            if norm_type == 'gn':\n                num_groups = 4  # Adjust according to your group size preference\n                identity_downsample = nn.Sequential(\n                    nn.Conv2d(self.in_channels, intermediate_channels * 2, kernel_size=1, stride=stride, bias=False),\n                    GroupNorm(num_features=intermediate_channels * 2, group=num_groups),\n                )\n            elif norm_type == 'bn':\n                identity_downsample = nn.Sequential(\n                    nn.Conv2d(self.in_channels, intermediate_channels * 2, kernel_size=1, stride=stride, bias=False),\n                    StandardBatchNorm(intermediate_channels * 2)\n                )\n            elif norm_type == 'in':\n                identity_downsample = nn.Sequential(\n                    nn.Conv2d(self.in_channels, intermediate_channels * 2, kernel_size=1, stride=stride, bias=False),\n                    InstanceNorm(intermediate_channels * 2)\n                )\n            elif norm_type == 'bin':\n                identity_downsample = nn.Sequential(\n                    nn.Conv2d(self.in_channels, intermediate_channels * 2, kernel_size=1, stride=stride, bias=False),\n                    BatchInstanceNorm(intermediate_channels * 2)\n                )\n            elif norm_type == 'ln':\n                identity_downsample = nn.Sequential(\n                    nn.Conv2d(self.in_channels, intermediate_channels * 2, kernel_size=1, stride=stride, bias=False),\n                    LayerNorm(intermediate_channels * 2)\n                )\n            elif norm_type == 'nn':\n                identity_downsample = nn.Sequential(\n                    nn.Conv2d(self.in_channels, intermediate_channels * 2, kernel_size=1, stride=stride, bias=False),\n                    IdentityNorm(intermediate_channels * 2)\n                )\n\n        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride, norm_type))\n        self.in_channels = intermediate_channels * 2\n\n        for i in range(num_residual_blocks - 1):\n            layers.append(block(self.in_channels, intermediate_channels, norm_type=norm_type))\n\n        return nn.Sequential(*layers)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:32.783993Z","iopub.execute_input":"2024-03-23T03:37:32.784659Z","iopub.status.idle":"2024-03-23T03:37:32.816898Z","shell.execute_reply.started":"2024-03-23T03:37:32.784623Z","shell.execute_reply":"2024-03-23T03:37:32.815994Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_loader","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:32.817979Z","iopub.execute_input":"2024-03-23T03:37:32.818326Z","iopub.status.idle":"2024-03-23T03:37:32.833729Z","shell.execute_reply.started":"2024-03-23T03:37:32.818295Z","shell.execute_reply":"2024-03-23T03:37:32.832927Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7f3689d95180>"},"metadata":{}}]},{"cell_type":"code","source":"val_loader","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:32.834818Z","iopub.execute_input":"2024-03-23T03:37:32.835097Z","iopub.status.idle":"2024-03-23T03:37:32.843639Z","shell.execute_reply.started":"2024-03-23T03:37:32.835057Z","shell.execute_reply":"2024-03-23T03:37:32.842799Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7f3689d95210>"},"metadata":{}}]},{"cell_type":"code","source":"import os\ndef train(model_name, n, batch_size, num_epochs, use_early_stopping, patience, num_classes, opt, lr,norm_type, num_workers,train_loader, val_loader):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n\n\n\n    num_epochs = num_epochs\n    num_classes= num_classes\n\n    def ResNetModel(img_channel=3, num_classes=25, norm_type=norm_type):\n        print(f\"\\nNorm in Train is: {norm_type}\\n\")\n        return ResNet(Block, [n,n,n], img_channel, num_classes, norm_type)\n\n\n    model = ResNetModel(img_channel=3, norm_type= norm_type,num_classes=25).to(device)\n    \n#     if torch.cuda.device_count() > 1:\n#         print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n#         model = nn.DataParallel(model)\n        \n    criterion = nn.CrossEntropyLoss()\n    if opt==\"SGD\":\n        optimizer = optim.SGD(model.parameters(), lr=lr)\n        \n    if opt==\"AdaGrad\":\n        optimizer = optim.Adagrad(model.parameters(), lr=lr)\n        \n    if opt==\"RMSprop\":\n        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n        \n    if opt==\"Adam\":\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        \n        \n    \n    model= model.to(device)\n    # Set the number of epochs\n    train_losses = []\n    train_accuracies = []\n    train_micro_f1_scores = []\n    train_macro_f1_scores = []\n    val_losses = []\n    val_accuracies = []\n    val_micro_f1_scores = []\n    val_macro_f1_scores = []\n\n    # Early stopping parameters\n    use_early_stopping = use_early_stopping\n    patience = patience \n    early_stopping_counter = 0\n    best_val_loss = np.inf\n    \n    folder_name = f'/kaggle/working/model_name_{model_name}'\n    \n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n        \n    csv_file_path = os.path.join(folder_name, f'{model_name}_metrics.csv')\n    \n    with open(csv_file_path, 'w') as f:\n        f.write(\"Epoch,Train Loss,Train Accuracy,Train Micro F1,Train Macro F1,Val Loss,Val Accuracy,Val Micro F1,Val Macro F1\\n\")\n        \n    \n\n    # Training loop\n    total_start_time = time.time()\n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        model.train()\n        running_loss = 0\n        all_labels = []\n        all_predictions = []\n\n        for data, labels in tqdm(train_loader, desc=f'Training - Epoch {epoch + 1}/{num_epochs}', leave=False):\n            x = data.to(device)\n            y = labels.to(device)\n\n            optimizer.zero_grad()\n            y_hat = model(x)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            all_labels.extend(y.cpu().numpy())\n            all_predictions.extend(torch.argmax(y_hat, dim=1).cpu().numpy())\n\n        epoch_train_time = time.time() - epoch_start_time\n        average_loss = running_loss / len(train_loader)\n        accuracy = accuracy_score(all_labels, all_predictions)\n        micro_f1 = f1_score(all_labels, all_predictions, average='micro')\n        macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n\n        print(f\"Train - Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}, Micro F1: {micro_f1:.4f}, Macro F1: {macro_f1:.4f}, Time: {epoch_train_time:.2f} seconds\")\n\n        train_losses.append(average_loss)\n        train_accuracies.append(accuracy)\n        train_micro_f1_scores.append(micro_f1)\n        train_macro_f1_scores.append(macro_f1)\n\n        epoch_val_start_time = time.time()\n        model.eval()\n        val_running_loss = 0\n        val_all_labels = []\n        val_all_predictions = []\n\n        with torch.no_grad():\n            for val_data, val_labels in tqdm(val_loader, desc=f'Validation - Epoch {epoch + 1}/{num_epochs}', leave=False):\n                val_x = val_data.to(device)\n                val_y = val_labels.to(device)\n                val_y_hat = model(val_x)\n                val_loss = criterion(val_y_hat, val_y)\n                val_running_loss += val_loss.item()\n                val_all_labels.extend(val_y.cpu().numpy())\n                val_all_predictions.extend(torch.argmax(val_y_hat, dim=1).cpu().numpy())\n\n        epoch_val_time = time.time() - epoch_val_start_time\n        val_average_loss = val_running_loss / len(val_loader)\n        val_accuracy = accuracy_score(val_all_labels, val_all_predictions)\n        val_micro_f1 = f1_score(val_all_labels, val_all_predictions, average='micro')\n        val_macro_f1 = f1_score(val_all_labels, val_all_predictions, average='macro')\n\n        print(f\"Validation - Epoch [{epoch + 1}/{num_epochs}], Loss: {val_average_loss:.4f}, Accuracy: {val_accuracy:.4f}, Micro F1: {val_micro_f1:.4f}, Macro F1: {val_macro_f1:.4f}, Time: {epoch_val_time:.2f} seconds\")\n\n        val_losses.append(val_average_loss)\n        val_accuracies.append(val_accuracy)\n        val_micro_f1_scores.append(val_micro_f1)\n        val_macro_f1_scores.append(val_macro_f1)\n\n        if use_early_stopping:\n            if val_average_loss < best_val_loss:\n                best_val_loss = val_average_loss\n                early_stopping_counter = 0\n            else:\n                early_stopping_counter += 1\n\n            if early_stopping_counter >= patience:\n                print(f\"Early stopping triggered after {epoch + 1} epochs without improvement.\")\n                break\n        \n        with open(csv_file_path, 'a') as f:\n            f.write(f\"{epoch + 1},{average_loss},{accuracy},{micro_f1},{macro_f1},{val_average_loss},{val_accuracy},{val_micro_f1},{val_macro_f1}\\n\")\n            \n            \n        print(f\"File created and saved successfully!!\\n\")\n            \n        torch.save(model.state_dict(), f'{folder_name}/{model_name}_model.pth')\n        print(\"Saved Model !!\")\n\n    total_train_time = time.time() - total_start_time\n    print(f\"Total Training Time: {total_train_time / 60:.2f} minutes\")\n\n\n\n\n\n    def plot_with_grid(x, train_data, val_data, train_label, val_label, xlabel, ylabel, title, x_interval=5, folder_name=None):\n        plt.figure(figsize=(8, 4))\n        plt.plot(x, train_data, label=train_label)\n        plt.plot(x, val_data, label=val_label)\n        plt.xlabel(xlabel)\n        plt.ylabel(ylabel)\n        plt.title(title)\n        plt.legend()\n        plt.xticks(np.arange(min(x), max(x)+1, x_interval))\n        plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n        \n        if folder_name is not None:\n            os.makedirs(folder_name, exist_ok=True)\n            file_path = os.path.join(folder_name, f\"{title.replace(' ', '_').lower()}.png\")\n            plt.savefig(file_path)\n        plt.show()\n\n    # Training Loss\n    plot_with_grid(range(len(train_losses)), train_losses, val_losses, 'Train', 'Validation', 'Epoch', 'Loss', 'Training and Validation Loss Curves', folder_name=folder_name)\n\n    # Training Accuracy\n    plot_with_grid(range(len(train_accuracies)), train_accuracies, val_accuracies, 'Train', 'Validation', 'Epoch', 'Accuracy', 'Training and Validation Accuracy Curves', folder_name=folder_name)\n\n    # Micro F1 Scores\n    plot_with_grid(range(len(train_micro_f1_scores)), train_micro_f1_scores, val_micro_f1_scores, 'Train Micro F1', 'Validation Micro F1', 'Epoch', 'F1 Score', 'Micro F1 Score Curves', folder_name=folder_name)\n\n    # Macro F1 Scores\n    plot_with_grid(range(len(train_macro_f1_scores)), train_macro_f1_scores, val_macro_f1_scores, 'Train Macro F1', 'Validation Macro F1', 'Epoch', 'F1 Score', 'Macro F1 Score Curves', folder_name=folder_name)\n    \n    return train_losses, train_accuracies, train_micro_f1_scores,train_macro_f1_scores, val_losses, val_accuracies,val_micro_f1_scores, val_macro_f1_scores","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:32.844879Z","iopub.execute_input":"2024-03-23T03:37:32.845209Z","iopub.status.idle":"2024-03-23T03:37:32.876768Z","shell.execute_reply.started":"2024-03-23T03:37:32.845180Z","shell.execute_reply":"2024-03-23T03:37:32.875728Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model_name= \"n2bs32ep50nc25optAdamlr-4normbnesF\"\nn =2\nbatch_size = 32\nnum_epochs = 50\nnum_classes = 25\nopt = \"Adam\"\nlr = 1e-4\nnorm_type = \"bn\"\nnum_workers = 4\nuse_early_stopping=  False\npatience=2","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:32.877850Z","iopub.execute_input":"2024-03-23T03:37:32.878080Z","iopub.status.idle":"2024-03-23T03:37:32.890857Z","shell.execute_reply.started":"2024-03-23T03:37:32.878060Z","shell.execute_reply":"2024-03-23T03:37:32.890074Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train(model_name, n, batch_size, num_epochs, use_early_stopping, patience, num_classes, opt, lr,norm_type, num_workers,train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T03:37:32.891836Z","iopub.execute_input":"2024-03-23T03:37:32.892092Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda\n\nNorm in Train is: bn\n\n","output_type":"stream"},{"name":"stderr","text":"Training - Epoch 1/50:  62%|██████▏   | 574/920 [03:35<02:08,  2.69it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}